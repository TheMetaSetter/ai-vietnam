{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Tokenization and Stop Words Removal using NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Động lực phát triển NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vào đầu thế kỷ 21, nhu cầu về các công cụ hỗ trợ nghiên cứu và giảng dạy NLP ngày càng tăng. Nhận thấy điều này, Steven Bird và Edward Loper từ Đại học Pennsylvania đã phát triển NLTK vào năm 2001. Mục tiêu của họ là tạo ra một bộ công cụ dễ sử dụng, giúp sinh viên và nhà nghiên cứu tiếp cận NLP một cách trực quan và hiệu quả.\n",
    "\n",
    "    | Steven Bird | Edward Loper |\n",
    "    |---------|---------|\n",
    "    | ![Steven Bird](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ4Zzcdzwd4Rhw_a8Bnl6NUg1c-fxk_MKIxPQ&s) | ![Edward Loper](https://www.github.com/edloper.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ra đời vào năm 2001, cho đến nay, NLTK đã trở thành một trong những thư viện NLP phổ biến nhất, được sử dụng rộng rãi trong cả học thuật và công nghiệp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Những điểm thú vị về NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Kho ngữ liệu phong phú:** NLTK cung cấp hơn 50 kho ngữ liệu (corpora) khác nhau, giúp người dùng dễ dàng thực hiện các tác vụ NLP đa dạng.\n",
    "\n",
    "- **Tính năng đa dạng:** Thư viện hỗ trợ nhiều chức năng như phân loại (classification), tách từ (tokenization), gán nhãn từ loại (POS tagging), phân tích cú pháp (parsing) và suy luận ngữ nghĩa (semantic reasoning).\n",
    "\n",
    "- **Cộng đồng lớn mạnh:** Với sự phổ biến của mình, NLTK có một cộng đồng người dùng và nhà phát triển đông đảo, liên tục đóng góp và cải tiến thư viện.\n",
    "\n",
    "  - GitHub repository của NLTK có đến hơn 13700 sao và hơn 2900 lượt fork. (27/12/2024)\n",
    "\n",
    "  - NLTK được sử dụng ở hơn 329000 public GitHub repo. (27/12/2024)\n",
    "\n",
    "  - Vào ngày 19 tháng 8 năm 2024, NLTK phiên bản 3.9 được released. Thì điều này nghĩa là thư viện này vẫn đang được phát triển và có lượng người dùng bền vững.\n",
    "\n",
    "- **Tài liệu học tập:** NLTK đi kèm với nhiều tài liệu hướng dẫn và ví dụ minh họa, giúp người mới bắt đầu dễ dàng tiếp cận và học hỏi.\n",
    "\n",
    "  - [Sách Natual Language Processing with Python](https://tjzhifei.github.io/resources/NLTK.pdf)\n",
    "\n",
    "  - [Tutorial về NLTK của trang Real Python](https://realpython.com/nltk-nlp-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Một số ứng dụng thực tế của NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Phân tích cảm xúc:** Xác định cảm xúc trong các đoạn văn bản, như đánh giá sản phẩm hoặc phản hồi của khách hàng.\n",
    "\n",
    "- **Tóm tắt văn bản:** Tạo ra các bản tóm tắt ngắn gọn từ các tài liệu dài.\n",
    "\n",
    "- **Nhận diện thực thể:** Xác định và phân loại các thực thể như tên người, địa điểm, tổ chức trong văn bản.\n",
    "\n",
    "- Trong phần demo này, mình sẽ tập trung vào bài toán phân loại cảm xúc (sentiment analysis). Bộ dữ liệu mình sử dụng có thể tìm thấy ở link sau: [SPOT](https://github.com/EdinburghNLP/spot-data/tree/master/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nội dung chính"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết từ NLTK, scikit-learn, và numpy\n",
    "# để chuẩn bị cho bài toán phân loại.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/conquerormikrokosmos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/conquerormikrokosmos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tải các tài nguyên của NLTK, bao gồm punkt tokenizer\n",
    "# và danh sácch các stop word.\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello there!', 'How are you doing today?', 'NLTK is pretty cool.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello there! How are you doing today? NLTK is pretty cool.\"\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', ',', ' ', 'N', 'L', 'T', 'K', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"Hello, NLTK!\"\n",
    "\n",
    "# Character tokenization\n",
    "char_tokens = list(text)\n",
    "\n",
    "char_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Byte-pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "# Set a pre-tokenizer to split input text into words\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Initialize a trainer with desired vocabulary size\n",
    "trainer = BpeTrainer(vocab_size=100, min_frequency=2)\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"This is a simple example to demonstrate Byte Pair Encoding.\",\n",
    "    \"Byte Pair Encoding is used in NLP for tokenization.\",\n",
    "    \"Tokenization is essential for processing text data.\"\n",
    "]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Byte', 'Pair', 'Encoding', 'is', 'e', 'f', 'f', 'e', 'c', 't', 'i', 'e', 'for', 't', 'okenization', '.']\n",
      "Token IDs: [50, 52, 58, 30, 10, 11, 11, 10, 8, 23, 14, 10, 54, 23, 57, 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode a new sentence\n",
    "encoding = tokenizer.encode(\"Byte Pair Encoding is effective for tokenization.\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", encoding.tokens)\n",
    "\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordPiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# Set a pre-tokenizer to split input text into words\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Initialize a trainer with desired vocabulary size\n",
    "trainer = WordPieceTrainer(vocab_size=100, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"Natural language processing is a fascinating field.\",\n",
    "    \"WordPiece tokenization helps in handling rare words.\",\n",
    "    \"Tokenization is essential for processing text data.\"\n",
    "]\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(corpus, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Tokenization', 'is', 'c', '##r', '##u', '##c', '##ial', 'for', '[UNK]', 't', '##a', '##s', '##k', '##s', '.']\n",
      "Token IDs: [74, 55, 11, 38, 44, 32, 91, 79, 0, 25, 30, 31, 42, 31, 5]\n"
     ]
    }
   ],
   "source": [
    "# Encode a new sentence\n",
    "encoding = tokenizer.encode(\"Tokenization is crucial for NLP tasks.\")\n",
    "\n",
    "# Print the tokens\n",
    "print(\"Tokens:\", encoding.tokens)\n",
    "\n",
    "# Print the token IDs\n",
    "print(\"Token IDs:\", encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dùng WordPiece Tokenizer được huấn luyện trước"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to convert output to tensors format pt, PyTorch or TensorFlow is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [101, 19204, 3989, 2003, 10232, 2005, 17953, 2361, 8518, 1012, 102]\n",
      "Tokens: [CLS]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encode a sentence\n",
    "encoding = tokenizer.encode_plus(\n",
    "    \"Tokenization is crucial for NLP tasks.\",\n",
    "    add_special_tokens=True,  # Adds [CLS] and [SEP] tokens\n",
    "    return_tensors='pt'       # Returns PyTorch tensors\n",
    ")\n",
    "\n",
    "# Print the input IDs\n",
    "print(\"Input IDs:\", encoding['input_ids'])\n",
    "\n",
    "# Print the tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khi nào dùng BPE và khi nào dùng WordPiece?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BPE:**\n",
    "- Khi cần tốc độ.\n",
    "- Khi corpus không lớn và ngữ cảnh không phức tạp.\n",
    "\n",
    "**WordPiece:**\n",
    "- Khi cần ngữ cảnh chính xác hơn.\n",
    "- Khi mô hình cần độ chính xác cao (như trong BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dùng danh sách các stop word của NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'C', 'L', ']']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize danh sách stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'C', 'L', ']']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_stop_words = stop_words.union({'quick', 'brown'})\n",
    "filtered_tokens_custom = [word for word in tokens if word.lower() not in custom_stop_words]\n",
    "filtered_tokens_custom"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
